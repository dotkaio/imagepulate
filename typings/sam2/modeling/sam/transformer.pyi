"""
This type stub file was generated by pyright.
"""

from typing import Tuple, Type
from torch import Tensor, nn

ALLOW_ALL_KERNELS = ...
def sdp_kernel_context(dropout_p): # -> nullcontext[None] | _GeneratorContextManager[dict[Any, Any], None, None]:
    """
    Get the context for the attention scaled dot-product kernel. We use Flash Attention
    by default, but fall back to all available kernels if Flash Attention fails.
    """
    ...

class TwoWayTransformer(nn.Module):
    def __init__(self, depth: int, embedding_dim: int, num_heads: int, mlp_dim: int, activation: Type[nn.Module] = ..., attention_downsample_rate: int = ...) -> None:
        """
        A transformer decoder that attends to an input image using
        queries whose positional embedding is supplied.

        Args:
          depth (int): number of layers in the transformer
          embedding_dim (int): the channel dimension for the input embeddings
          num_heads (int): the number of heads for multihead attention. Must
            divide embedding_dim
          mlp_dim (int): the channel dimension internal to the MLP block
          activation (nn.Module): the activation to use in the MLP block
        """
        ...
    
    def forward(self, image_embedding: Tensor, image_pe: Tensor, point_embedding: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
          image_embedding (torch.Tensor): image to attend to. Should be shape
            B x embedding_dim x h x w for any h and w.
          image_pe (torch.Tensor): the positional encoding to add to the image. Must
            have the same shape as image_embedding.
          point_embedding (torch.Tensor): the embedding to add to the query points.
            Must have shape B x N_points x embedding_dim for any N_points.

        Returns:
          torch.Tensor: the processed point_embedding
          torch.Tensor: the processed image_embedding
        """
        ...
    


class TwoWayAttentionBlock(nn.Module):
    def __init__(self, embedding_dim: int, num_heads: int, mlp_dim: int = ..., activation: Type[nn.Module] = ..., attention_downsample_rate: int = ..., skip_first_layer_pe: bool = ...) -> None:
        """
        A transformer block with four layers: (1) self-attention of sparse
        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp
        block on sparse inputs, and (4) cross attention of dense inputs to sparse
        inputs.

        Arguments:
          embedding_dim (int): the channel dimension of the embeddings
          num_heads (int): the number of heads in the attention layers
          mlp_dim (int): the hidden dimension of the mlp block
          activation (nn.Module): the activation of the mlp block
          skip_first_layer_pe (bool): skip the PE on the first layer
        """
        ...
    
    def forward(self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor) -> Tuple[Tensor, Tensor]:
        ...
    


class Attention(nn.Module):
    """
    An attention layer that allows for downscaling the size of the embedding
    after projection to queries, keys, and values.
    """
    def __init__(self, embedding_dim: int, num_heads: int, downsample_rate: int = ..., dropout: float = ..., kv_in_dim: int = ...) -> None:
        ...
    
    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:
        ...
    


class RoPEAttention(Attention):
    """Attention with rotary position encoding."""
    def __init__(self, *args, rope_theta=..., rope_k_repeat=..., feat_sizes=..., **kwargs) -> None:
        ...
    
    def forward(self, q: Tensor, k: Tensor, v: Tensor, num_k_exclude_rope: int = ...) -> Tensor:
        ...
    


